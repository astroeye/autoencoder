# 과대적합과 과소적합

# 지금까지 그랬듯이 이 예제의 코드도 tf.keras API를 사용합니다.
#  텐서플로 케라스 가이드에서 tf.keras API에 대해 더 많은 정보를 얻을 수 있습니다.

# 앞서 영화 리뷰 분류와 주택 가격 예측의 두 예제에서 일정 에포크 동안 훈련하면
#  검증 세트에서 모델 성능이 최고점에 도달한 다음 감소하기 시작한 것을 보았습니다.

# 다른 말로 하면, 모델이 훈련 세트에 과대적합(overfitting)된 것입니다.
#  과대적합을 다루는 방법은 꼭 배워야 합니다. 훈련 세트에서 높은 성능을 얻을 수 있지만
#  진짜 원하는 것은 테스트 세트(또는 이전에 본 적 없는 데이터)에 잘 일반화되는 모델입니다.

# 과대적합의 반대는 과소적합(underfitting)입니다. 과소적합은 테스트 세트의 성능이
#  향상될 여지가 아직 있을 때 일어납니다. 발생하는 원인은 여러가지입니다.
#  모델이 너무 단순하거나, 규제가 너무 많거나, 그냥 단순히 충분히 오래 훈련하지 않는 경우입니다.
#  즉 네트워크가 훈련 세트에서 적절한 패턴을 학습하지 못했다는 뜻입니다.

# 모델을 너무 오래 훈련하면 과대적합되기 시작하고 테스트 세트에서 일반화되지 못하는 패턴을
#  훈련 세트에서 학습합니다. 과대적합과 과소적합 사이에서 균형을 잡아야 합니다.
#  이를 위해 적절한 에포크 횟수동안 모델을 훈련하는 방법을 배워보겠습니다.

# 과대적합을 막는 가장 좋은 방법은 더 많은 훈련 데이터를 사용하는 것입니다.
#  많은 데이터에서 훈련한 모델은 자연적으로 일반화 성능이 더 좋습니다.
#  데이터를 더 준비할 수 없을 때 그다음으로 가장 좋은 방법은 규제(regularization)와 같은 기법을
#  사용하는 것입니다. 모델이 저장할 수 있는 정보의 양과 종류에 제약을 부과하는 방법입니다.
#  네트워크가 소수의 패턴만 기억할 수 있다면 최적화 과정 동안 일반화 가능성이 높은
#  가장 중요한 패턴에 촛점을 맞출 것입니다.

# 이 노트북에서 널리 사용되는 두 가지 규제 기법인 가중치 규제와 드롭아웃(dropout)을 알아 보겠습니다.
#  이런 기법을 사용하여 IMDB 영화 리뷰 분류 모델의 성능을 향상시켜 보죠.

import tensorflow as tf
from tensorflow import keras

import numpy as np
import matplotlib.pyplot as plt

print(tf.__version__)
# 2.8.0

# IMDB 데이터셋 다운로드
# 이전 노트북에서처럼 임베딩을 사용하지 않고 여기에서는 문장을 멀티-핫 인코딩
# (multi-hot encoding)으로 변환하겠습니다. 이 모델은 훈련 세트에 빠르게 과대적합될 것입니다.
#  과대적합을 발생시키기고 어떻게 해결하는지 보이기 위해 선택했습니다.

# 멀티-핫 인코딩은 정수 시퀀스를 0과 1로 이루어진 벡터로 변환합니다.
#  정확하게 말하면 시퀀스 [3, 5]를 인덱스 3과 5만 1이고 나머지는 모두 0인 10,000 차원 벡터로
#  변환한다는 의미입니다.

NUM_WORDS = 1000

(train_data, train_labels), (test_data, test_labels) = keras.datasets.imdb.load_data(num_words=NUM_WORDS)

print(train_data.shape, type(train_data[0]), len(train_data[0]))
# (25000,) <class 'list'> 218
print(train_labels.shape, type(train_labels[0]), train_labels[0].shape)
# (25000,) <class 'numpy.int64'> ()

# 리뷰 텍스트는 어휘 사전의 특정 단어를 나타내는 정수로 변환되어 있습니다.
#  첫 번째 리뷰를 확인해 보겠습니다:
print(train_data[0])
# [1, 14, 22, 16, 43, 530, 973, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100,
#  43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385,
#  39, 4, 172, 2, 2, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4,
#  2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 2, 4, 22, 17, 515, 17,
#  12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2,
#  33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12,
#  215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5,
#  723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15,
#  297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5,
#  144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16,
#  283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]

# 영화 리뷰들은 길이가 다릅니다.
#  다음 코드는 첫 번째 리뷰와 두 번째 리뷰에서 단어의 개수를 출력합니다.
#  신경망의 입력은 길이가 같아야 하기 때문에 나중에 이 문제를 해결하겠습니다.
len(train_data[0]), len(train_data[1])
# (218, 189)

print(test_data.shape, type(test_data[0]), len(test_data[0]))
# (25000,) <class 'list'> 68
print(test_labels.shape, type(test_labels[0]), test_labels[0].shape)
# (25000,) <class 'numpy.int64'> ()


def multi_hot_sequences(sequences, dimension):
    # 0으로 채워진 (len(sequences), dimension) 크기의 행렬을 만듭니다
    results = np.zeros((len(sequences), dimension))
    for i, word_indices in enumerate(sequences):
        results[i, word_indices] = 1.0  # results[i]의 특정 인덱스만 1로 설정합니다
    return results

train_data = multi_hot_sequences(train_data, dimension=NUM_WORDS)
test_data = multi_hot_sequences(test_data, dimension=NUM_WORDS)




















































